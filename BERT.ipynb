{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow_hub\n",
    "#!pip install tensorflow_text\n",
    "#!pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "all_df = pd.read_csv('./kaggle_train_dataset.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "all_df[\"content\"] = all_df[\"content\"].fillna(\"NoName\")\n",
    "all_df[\"text\"] = all_df.title + all_df.content\n",
    "\n",
    "texts = all_df[\"text\"].values\n",
    "labels = all_df[\"label\"].values\n",
    "mydict = {'informative':\"0\", 'happy':\"1\", 'angry':\"2\", 'depressing':\"3\", 'odd':\"4\", 'boring':\"5\", 'warm':\"6\", 'worried':\"7\"}\n",
    "final_label = []\n",
    "for i in labels:\n",
    "    final_label.append(mydict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts, final_label, test_size=0.2, random_state=5)\n",
    "y_trainOneHot = utils.to_categorical(y_train)\n",
    "y_testOneHot = utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "tfhub_handle_preprocess = \"https://hub.tensorflow.google.cn/tensorflow/bert_zh_preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://hub.tensorflow.google.cn/tensorflow/bert_zh_L-12_H-768_A-12/4\"\n",
    "    \n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.5)(net)\n",
    "    net = tf.keras.layers.Dense(8, activation='softmax', name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "model = build_classifier_model()\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "num_train_steps= int(len(x_train) / batch_size * epochs)\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "optimizer = optimization.create_optimizer(init_lr=3e-5, \n",
    "                                          num_train_steps=num_train_steps, \n",
    "                                          num_warmup_steps=num_warmup_steps, \n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "              metrics=tf.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 1.5221 - categorical_accuracy: 0.4995\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.62053, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 210s 322ms/step - loss: 1.5221 - categorical_accuracy: 0.4995 - val_loss: 1.0414 - val_categorical_accuracy: 0.6205\n",
      "Epoch 2/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 1.0064 - categorical_accuracy: 0.6498\n",
      "Epoch 2: val_categorical_accuracy improved from 0.62053 to 0.67324, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 199s 324ms/step - loss: 1.0064 - categorical_accuracy: 0.6498 - val_loss: 0.9476 - val_categorical_accuracy: 0.6732\n",
      "Epoch 3/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.7283 - categorical_accuracy: 0.7502\n",
      "Epoch 3: val_categorical_accuracy improved from 0.67324 to 0.68744, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 200s 326ms/step - loss: 0.7283 - categorical_accuracy: 0.7502 - val_loss: 0.9529 - val_categorical_accuracy: 0.6874\n",
      "Epoch 4/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.4758 - categorical_accuracy: 0.8403\n",
      "Epoch 4: val_categorical_accuracy improved from 0.68744 to 0.70761, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 200s 326ms/step - loss: 0.4758 - categorical_accuracy: 0.8403 - val_loss: 0.9732 - val_categorical_accuracy: 0.7076\n",
      "Epoch 5/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2939 - categorical_accuracy: 0.9024\n",
      "Epoch 5: val_categorical_accuracy improved from 0.70761 to 0.71219, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 198s 323ms/step - loss: 0.2939 - categorical_accuracy: 0.9024 - val_loss: 1.1873 - val_categorical_accuracy: 0.7122\n",
      "Epoch 6/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1791 - categorical_accuracy: 0.9420\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.71219\n",
      "614/614 [==============================] - 188s 306ms/step - loss: 0.1791 - categorical_accuracy: 0.9420 - val_loss: 1.4753 - val_categorical_accuracy: 0.7081\n",
      "Epoch 7/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1144 - categorical_accuracy: 0.9650\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.71219\n",
      "614/614 [==============================] - 188s 306ms/step - loss: 0.1144 - categorical_accuracy: 0.9650 - val_loss: 1.6666 - val_categorical_accuracy: 0.7094\n",
      "Epoch 8/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.0715 - categorical_accuracy: 0.9785\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.71219\n",
      "614/614 [==============================] - 189s 308ms/step - loss: 0.0715 - categorical_accuracy: 0.9785 - val_loss: 2.0001 - val_categorical_accuracy: 0.7099\n",
      "Epoch 9/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.0445 - categorical_accuracy: 0.9865\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.71219\n",
      "614/614 [==============================] - 189s 308ms/step - loss: 0.0445 - categorical_accuracy: 0.9865 - val_loss: 2.2941 - val_categorical_accuracy: 0.7085\n",
      "Epoch 10/10\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.0283 - categorical_accuracy: 0.9919\n",
      "Epoch 10: val_categorical_accuracy improved from 0.71219 to 0.72090, saving model to weights.best.hdf5\n",
      "614/614 [==============================] - 202s 329ms/step - loss: 0.0283 - categorical_accuracy: 0.9919 - val_loss: 2.4520 - val_categorical_accuracy: 0.7209\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "filepath=\"weights.best.hdf5\"\n",
    "callbacks_list = [\n",
    "    EarlyStopping(verbose=True, patience=5, monitor='val_categorical_accuracy'),\n",
    "    ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "train_history = model.fit(x_train, y_trainOneHot, \n",
    "                          batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "                          validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 27s 155ms/step\n",
      "Macro-average: 0.7045545490902929\n",
      "Micro-average: 0.7244729605866178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.76      0.77      2373\n",
      "         1.0       0.64      0.67      0.66      1047\n",
      "         2.0       0.69      0.70      0.69       722\n",
      "         3.0       0.77      0.79      0.78       393\n",
      "         4.0       0.65      0.70      0.68       365\n",
      "         5.0       0.72      0.65      0.68       334\n",
      "         6.0       0.77      0.80      0.78       163\n",
      "         7.0       0.57      0.62      0.60        58\n",
      "\n",
      "    accuracy                           0.72      5455\n",
      "   macro avg       0.70      0.71      0.70      5455\n",
      "weighted avg       0.73      0.72      0.73      5455\n",
      "\n",
      "[[1793  266  128   47   49   39   32   19]\n",
      " [ 221  704   46   22   24   23    5    2]\n",
      " [ 115   41  505    7   40   12    0    2]\n",
      " [  40   15   12  311    8    4    2    1]\n",
      " [  46   24   27    2  257    6    0    3]\n",
      " [  48   42   14    6    8  216    0    0]\n",
      " [  15    9    0    5    2    2  130    0]\n",
      " [  14    1    0    2    5    0    0   36]]\n",
      "0.7244729605866178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "result = model.predict(x_test).argmax(axis=-1)\n",
    "result = np.array(result, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(y_test, result, average = 'macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(y_test, result, average = 'micro')))\n",
    "print(metrics.classification_report(y_test, result))\n",
    "print(metrics.confusion_matrix(y_test, result))\n",
    "print(metrics.accuracy_score(y_test, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./kaggle_test_dataset.csv', sep='\\t', encoding='utf-8')\n",
    "df_test[\"content\"] = df_test[\"content\"].fillna(\"NoName\")\n",
    "df_test[\"text\"] = df_test.title + df_test.content\n",
    "TEST_features = df_test[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 98s 155ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "TEST_predict_result = model.predict(TEST_features).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./kaggle_submission.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f'Id,Label\\n')\n",
    "    for i in range(len(TEST_predict_result)):\n",
    "        f.write(f'{i},{TEST_predict_result[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
